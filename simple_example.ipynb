{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTTE-RNN in keras\n",
    "\n",
    "Building upon the excellent post by Dayne Batten, I finally managed to get a hunch about how to get it working in keras. \n",
    "\n",
    "I didn't try long but couldn't get [exp,softplus] output layer working so I'm using only exp.\n",
    "\n",
    "https://github.com/daynebatten/keras-wtte-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as k\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 2) (10, 30, 1)\n",
      "(10, 30, 2) (10, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Discrete log-likelihood for Weibull hazard function on censored survival data\n",
    "    y_true is a (samples, 2) tensor containing time-to-event (y), and an event indicator (u)\n",
    "    ab_pred is a (samples, 2) tensor containing predicted Weibull alpha (a) and beta (b) parameters\n",
    "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
    "\"\"\"\n",
    "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "\n",
    "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
    "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
    "\n",
    "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)\n",
    "\n",
    "\"\"\"\n",
    "    Not used for this model, but included in case somebody needs it\n",
    "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
    "\"\"\"\n",
    "def weibull_loglik_continuous(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "\n",
    "    ya = (y_ + 1e-35) / a_\n",
    "    return -1 * k.mean(u_ * (k.log(b_) + b_ * k.log(ya)) - k.pow(ya, b_))\n",
    "\n",
    "def get_data(n,every_nth):\n",
    "    # create some simple data of evenly spaced events recurring every_nth step\n",
    "    events = np.array([np.array(xrange(n)) for _ in xrange(every_nth)])+np.array(xrange(every_nth)).reshape(every_nth,1)+1\n",
    "    true_time_to_event = every_nth-1-events%every_nth\n",
    "\n",
    "    was_event = (events%every_nth==0)*1.0\n",
    "    was_event[:,0] = 0.5\n",
    "\n",
    "    events =  true_time_to_event==0\n",
    "\n",
    "    is_censored  = (events[:,::-1].cumsum(1)[:,::-1]==0)*1 # Always works (?)\n",
    "    censored_tte = is_censored[:,::-1].cumsum(1)[:,::-1]*is_censored+(1-is_censored)*true_time_to_event\n",
    "    events = np.copy(events.T*1.0)\n",
    "    true_time_to_event = np.copy(true_time_to_event.T*1.0)\n",
    "    censored_tte = np.copy(censored_tte.T*1.0)\n",
    "    was_event = np.copy(was_event.T*1.0)\n",
    "    \n",
    "    not_censored = 1-np.copy(is_censored.T*1.0)\n",
    "    \n",
    "    return censored_tte,not_censored,was_event,events,true_time_to_event\n",
    "\n",
    "seq_len    = 30\n",
    "batch_size = every_nth = 10\n",
    "n_features = 1\n",
    "\n",
    "censored_tte,not_censored,was_event,events,true_time_to_event = get_data(n=seq_len,every_nth=every_nth)\n",
    "\n",
    "# From https://keras.io/layers/recurrent/\n",
    "# input shape rnn recurrent: (nb_samples, timesteps, input_dim)\n",
    "# input shape rnn recurrent: \n",
    "#    if return_sequences: 3D tensor with shape (nb_samples, timesteps, output_dim).\n",
    "#    else, 2D tensor with shape (nb_samples, output_dim).\n",
    "\n",
    "train_u  = np.expand_dims(not_censored.T,axis=3)      # (batch_size,seq_len,1)\n",
    "train_x  = np.expand_dims(was_event.T,axis=3)         # (batch_size,seq_len,1)\n",
    "censored_tte = np.expand_dims(censored_tte.T, axis=3) # (batch_size,seq_len,1)\n",
    "train_y = np.append(censored_tte,train_u,axis=2) # (batch_size,seq_len,2)\n",
    "# train_y \n",
    "print train_y.shape,train_x.shape\n",
    "\n",
    "test_u  = np.ones_like(train_u)                      # (batch_size,seq_len,1)\n",
    "test_x  = train_x                                    # (batch_size,seq_len,1)\n",
    "true_time_to_event = np.expand_dims(true_time_to_event.T, axis=3) # (batch_size,seq_len,1)\n",
    "test_y = np.append(censored_tte,censored_tte,axis=2) # (batch_size,seq_len,2)\n",
    "print test_y.shape,test_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Here's the rest of the meat of the demo... actually fitting and training the model.\n",
    "    We'll also make some test predictions so we can evaluate model performance.\n",
    "\"\"\"\n",
    "\n",
    "# Start building our model\n",
    "model = Sequential()\n",
    "\n",
    "# Mask parts of the lookback period that are all zeros (i.e., unobserved) so they don't skew the model\n",
    "#model.add(Masking(mask_value=0., input_shape=(max_time, n_features)))\n",
    "\n",
    "# LSTM is just a common type of RNN. You could also try anything else (e.g., GRU).\n",
    "model.add(LSTM(5, input_shape=(seq_len, n_features),return_sequences=True))\n",
    "\n",
    "# We need 2 neurons to output Alpha and Beta parameters for our Weibull distribution\n",
    "model.add(Dense(output_dim=2))\n",
    "\n",
    "# Add a positive output layer\n",
    "model.add(Activation(k.exp))\n",
    "\n",
    "# Use the discrete log-likelihood for Weibull survival data as our loss function\n",
    "model.compile(loss=weibull_loglik_discrete, optimizer=RMSprop(lr=.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit!\n",
    "model.fit(train_x, train_y, nb_epoch=5, batch_size=batch_size/2, verbose=2, validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make some parametric predictions\n",
    "test_predict = model.predict(test_x)\n",
    "\n",
    "# TTE, Event Indicator, Alpha, Beta\n",
    "print(test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results: It's learning something!!\n",
    "batch_indx = 0\n",
    "a = test_predict[batch_indx,:,0]\n",
    "b = test_predict[batch_indx,:,1]\n",
    "plt.plot(a)\n",
    "plt.title('predicted alpha')\n",
    "plt.show()\n",
    "plt.plot(b)\n",
    "plt.title('predicted beta')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(xrange(seq_len),events.T[batch_indx,:])\n",
    "plt.title('event')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But it's horribly out of scale. More work needed :) \n",
    "def weibull_mean(a, b):\n",
    "    # Continuous mean. theoretically at most 1 step below discretized mean \n",
    "    # E[T ] â‰¤ E[Td] + 1 true for positive distributions. \n",
    "    # Might be numerically unstable\n",
    "    from scipy.special import gamma\n",
    "    return a*gamma(1.0+1.0/b)\n",
    "\n",
    "plt.plot(weibull_mean(a,b),label='predicted')\n",
    "plt.plot(test_y[batch_indx,:,0] ,label='actual')\n",
    "plt.plot(train_y[batch_indx,:,0],label='censored')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
